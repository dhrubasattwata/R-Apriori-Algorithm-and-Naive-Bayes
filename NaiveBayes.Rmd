---
title: "Naive Bayes"
author: "Dhrubasattwata Roy Choudhury"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Naive Bayes

Naive Bayes is a Supervised Machine Learning algorithm based on the Bayes Theorem that is used to solve classification problems by following a probabilistic approach. It is based on the idea that the predictor variables in a Machine Learning model are independent of each other. Meaning that the outcome of a model depends on a set of independent variables that have nothing to do with each other.   

In real-world problems, predictor variables aren’t always independent of each other, there are always some correlations between them. Since Naive Bayes considers each predictor variable to be independent of any other variable in the model, it is called ‘Naive’.

Now let’s understand the logic behind the Naive Bayes algorithm.

The principle behind Naive Bayes is the Bayes theorem also known as the Bayes Rule. The Bayes theorem is used to calculate the conditional probability, which is nothing but the probability of an event occurring based on information about the events in the past.Therefore, the Bayes theorem can be summed up as:   
Posterior=(Likelihood).(Proposition prior probability)/Evidence prior probability

```{r}
# Import the packages
library(tidyverse)
library(ggplot2)
library(caret)
library(caretEnsemble)
library(Amelia)
library(mice)
library(GGally)
library(rpart)
library(randomForest)
```

### Datasets

You can also embed plots, for example:

```{r}
data<- read.csv("C:/Users/Dhruba/Documents/R/data/diabetes.csv")
```

Before we study the data set let’s convert the output variable (‘Outcome’) into a categorical variable. This is necessary because our output will be in the form of 2 classes, True or False. Where true will denote that a patient has diabetes and false denotes that a person is diabetes free
```{r}
#Setting outcome variables as categorical
data$Outcome <- factor(data$Outcome, levels = c(0,1), labels = c("False", "True"))
```

### Data Exploration
```{r}
str(data)
head(data)
#describe(data)
```

### Data Cleaning
```{r}
#Convert '0' values into NA
data[, 2:7][data[, 2:7] == 0] <- NA
```

### Visualize
```{r}
#visualize the missing data
missmap(data)
```

The above illustrations show that our data set has plenty missing values and removing all of them will leave us with an even smaller data set, therefore, we can perform imputations by using the mice package in R.

```{r}
#Use mice package to predict missing values
mice_mod <- mice(data[, c("Glucose","BloodPressure","SkinThickness","Insulin","BMI")], method='rf')
mice_complete <- complete(mice_mod)

#Transfer the predicted missing values into the main data set
data$Glucose <- mice_complete$Glucose
data$BloodPressure <- mice_complete$BloodPressure
data$SkinThickness <- mice_complete$SkinThickness
data$Insulin<- mice_complete$Insulin
data$BMI <- mice_complete$BMI
```

```{r}
#visualize the missing data
missmap(data)
```

### Exploratory Data Analysis

Now let’s perform a couple of visualizations to take a better look at each variable, this stage is essential to understand the significance of each predictor variable.

```{r}
#Data Visualization
#Visual 1
ggplot(data, aes(Age, colour = Outcome)) +
geom_freqpoly(binwidth = 1) + labs(title="Age Distribution by Outcome")
```

```{r}
#visual 2
c <- ggplot(data, aes(x=Pregnancies, fill=Outcome, color=Outcome)) +
geom_histogram(binwidth = 1) + labs(title="Pregnancy Distribution by Outcome")
c + theme_bw()
```

```{r}
#visual 3
P <- ggplot(data, aes(x=BMI, fill=Outcome, color=Outcome)) +
geom_histogram(binwidth = 1) + labs(title="BMI Distribution by Outcome")
P + theme_bw()
```

```{r}
#visual 4
ggplot(data, aes(Glucose, colour = Outcome)) +
geom_freqpoly(binwidth = 1) + labs(title="Glucose Distribution by Outcome")
```

```{r}
#visual 5
ggpairs(data)
```

### Data Modelling
This stage begins with a process called Data Splicing, wherein the data set is split into two parts:  

1. Training set: This part of the data set is used to build and train the Machine Learning model.  
2. Testing set: This part of the data set is used to evaluate the efficiency of the model. 

```{r}
#Building a model
#split data into training and test data sets
indxTrain <- createDataPartition(y = data$Outcome,p = 0.75,list = FALSE)
training <- data[indxTrain,]
testing <- data[-indxTrain,] #Check dimensions of the split > prop.table(table(data$Outcome)) * 100

prop.table(table(training$Outcome)) * 100
prop.table(table(testing$Outcome)) * 100
```

For comparing the outcome of the training and testing phase let’s create separate variables that store the value of the response variable:
```{r}
#create objects x which holds the predictor variables and y which holds the response variables
x = training[,-9]
y = training$Outcome
```

Now it’s time to load the e1071 package that holds the Naive Bayes function. This is an in-built function provided by R.
```{r}
library(e1071)
library(klaR)

model = suppressWarnings(train(x,y,'nb',trControl=trainControl(method='cv',number=10)))
model
```
We thus created a predictive model by using the Naive Bayes Classifier.

### Model Evaluation

To check the efficiency of the model, we are now going to run the testing data set on the model, after which we will evaluate the accuracy of the model by using a Confusion matrix.

```{r}
#Model Evaluation
#Predict testing set
Predict <- suppressWarnings(predict(model,newdata = testing )) #Get the confusion matrix to see accuracy value and other parameter values
confusionMatrix(Predict, testing$Outcome )
```

The final output shows that we built a Naive Bayes classifier that can predict whether a person is diabetic or not, with an accuracy of approximately 73%.

To summaries the demo, let’s draw a plot that shows how each predictor variable is independently responsible for predicting the outcome.

```{r}
#Plot Variable performance
X <- varImp(model)
plot(X)
```

































